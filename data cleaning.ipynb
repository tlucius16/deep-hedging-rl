{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a786df96-a53c-4fb9-bf72-5629cb745318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load pipeline output\n",
    "df = pd.read_csv(\"data/processed/market_daily.csv\", parse_dates=True, index_col=0)\n",
    "\n",
    "# Drop any rows with missing data\n",
    "df_clean = df.dropna(how=\"any\")\n",
    "\n",
    "# Save clean versions\n",
    "df_clean.to_csv(\"data/processed/market_daily_clean.csv\")\n",
    "df_clean.to_parquet(\"data/processed/market_daily_clean.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d2914c6-a61c-4227-98fe-ce7e44e649df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned market_daily.csv\n",
      "‚úÖ Cleaned market_daily.parquet\n",
      "‚úÖ Cleaned market_extended_spx.csv\n",
      "‚úÖ Cleaned market_extended_spx.parquet\n",
      "‚úÖ Cleaned market_extended_spy.csv\n",
      "‚úÖ Cleaned market_extended_spy.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22308\\2187554634.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(fpath, parse_dates=True, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned options_snapshot_spx.csv\n",
      "‚úÖ Cleaned options_snapshot_spx.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22308\\2187554634.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(fpath, parse_dates=True, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned options_snapshot_spy.csv\n",
      "‚úÖ Cleaned options_snapshot_spy.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22308\\2187554634.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(fpath, parse_dates=True, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned vol_surface_spx.csv\n",
      "‚úÖ Cleaned vol_surface_spx.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22308\\2187554634.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(fpath, parse_dates=True, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned vol_surface_spy.csv\n",
      "‚úÖ Cleaned vol_surface_spy.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "processed_path = \"data/processed\"\n",
    "cleaned_path = os.path.join(processed_path, \"cleaned\")\n",
    "os.makedirs(cleaned_path, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(processed_path):\n",
    "    if (\"clean\" in fname.lower()) or fname.startswith(\".\") or fname.endswith(\".txt\"):\n",
    "        continue  # skip already-cleaned, hidden, or placeholder files\n",
    "\n",
    "    fpath = os.path.join(processed_path, fname)\n",
    "    base, ext = os.path.splitext(fname)\n",
    "\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(fpath, parse_dates=True, index_col=0)\n",
    "    elif ext == \".parquet\":\n",
    "        df = pd.read_parquet(fpath)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Drop NaNs\n",
    "    df_clean = df.dropna(how=\"any\")\n",
    "\n",
    "    # Save as both CSV + Parquet\n",
    "    df_clean.to_csv(os.path.join(cleaned_path, f\"{base}_clean.csv\"))\n",
    "    df_clean.to_parquet(os.path.join(cleaned_path, f\"{base}_clean.parquet\"))\n",
    "\n",
    "    print(f\"‚úÖ Cleaned {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137de5db-549c-4a89-b20c-67c27414fe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ options_snapshot_spx: 6,963 rows, 20 cols\n",
      "‚úÖ vol_surface_spx: 6,963 rows, 11 cols\n",
      "‚úÖ market_extended_spx: 6,963 rows, 24 cols\n",
      "‚úÖ market_daily: 10,831 rows, 6 cols\n",
      "\n",
      "üì¶ SPX combined saved:\n",
      "   - data\\processed\\aligned\\spx\\combined_spx_all.csv\n",
      "   - data\\processed\\aligned\\spx\\combined_spx_all.parquet\n",
      "   Shape: 31,720 rows √ó 44 cols\n",
      "   Date range: 1996-01-04 00:00:00 ‚Üí 2025-08-29 00:00:00\n",
      "\n",
      "Row counts by source:\n",
      "source\n",
      "market_daily            10831\n",
      "options_snapshot_spx     6963\n",
      "vol_surface_spx          6963\n",
      "market_extended_spx      6963\n",
      "‚úÖ options_snapshot_spy: 4,692 rows, 20 cols\n",
      "‚úÖ vol_surface_spy: 4,693 rows, 11 cols\n",
      "‚úÖ market_extended_spy: 4,693 rows, 24 cols\n",
      "‚úÖ market_daily: 7,546 rows, 6 cols\n",
      "\n",
      "üì¶ SPY combined saved:\n",
      "   - data\\processed\\aligned\\spy\\combined_spy_all.csv\n",
      "   - data\\processed\\aligned\\spy\\combined_spy_all.parquet\n",
      "   Shape: 21,624 rows √ó 44 cols\n",
      "   Date range: 2005-01-01 00:00:00 ‚Üí 2025-08-29 00:00:00\n",
      "\n",
      "Row counts by source:\n",
      "source\n",
      "market_daily            7546\n",
      "vol_surface_spy         4693\n",
      "market_extended_spy     4693\n",
      "options_snapshot_spy    4692\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CLEANED = \"data/processed/cleaned\"\n",
    "ALIGNED = os.path.join(\"data\", \"processed\", \"aligned\")\n",
    "os.makedirs(ALIGNED, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "def load_csv_with_date(path):\n",
    "    \"\"\"Read a CSV and ensure there is a proper 'date' column of dtype datetime64[ns].\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    else:\n",
    "        # fall back: treat first column as date-like index\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "        df = df.reset_index().rename(columns={\"index\": \"date\"})\n",
    "    # drop rows with invalid dates\n",
    "    df = df[df[\"date\"].notna()]\n",
    "    # sort & dedup on date if needed (keep first for identical duplicates)\n",
    "    df = df.sort_values(\"date\")\n",
    "    df = df[~df[\"date\"].duplicated(keep=\"first\")] if df[\"date\"].is_unique is False else df\n",
    "    return df\n",
    "\n",
    "def load_market(start_date=None):\n",
    "    mkt_path = os.path.join(CLEANED, \"market_daily_clean.csv\")\n",
    "    mkt = load_csv_with_date(mkt_path)\n",
    "    if start_date:\n",
    "        mkt = mkt[mkt[\"date\"] >= pd.to_datetime(start_date)]\n",
    "    return mkt\n",
    "\n",
    "def row_preserving_join_with_market(df, market_df):\n",
    "    \"\"\"Inner-join market columns onto df by date, preserving all df rows for those dates.\"\"\"\n",
    "    # avoid column collisions: we won't rename, just rely on union; market cols have distinct names (close_spy, close_gspc, etc.)\n",
    "    merged = df.merge(market_df, on=\"date\", how=\"inner\")\n",
    "    return merged\n",
    "\n",
    "def safe_load(path):\n",
    "    if os.path.exists(path):\n",
    "        return load_csv_with_date(path)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping (not found): {path}\")\n",
    "        return None\n",
    "\n",
    "def build_combined(symbol, start_date):\n",
    "    \"\"\"\n",
    "    Build one combined table for a symbol ('spx' or 'spy'):\n",
    "      - load market_daily (trimmed to start_date)\n",
    "      - load symbol-specific cleaned datasets\n",
    "      - merge market cols into each dataset by date (row-preserving)\n",
    "      - concatenate all rows together (add 'source' column), keep ALL columns\n",
    "    \"\"\"\n",
    "    out_dir = os.path.join(ALIGNED, symbol)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # market (trimmed)\n",
    "    mkt = load_market(start_date=start_date)\n",
    "\n",
    "    # expected cleaned inputs for each symbol\n",
    "    parts = []\n",
    "    sources = []\n",
    "\n",
    "    if symbol == \"spx\":\n",
    "        files = [\n",
    "            (\"options_snapshot_spx_clean.csv\", \"options_snapshot_spx\"),\n",
    "            (\"vol_surface_spx_clean.csv\",     \"vol_surface_spx\"),\n",
    "            (\"market_extended_spx_clean.csv\", \"market_extended_spx\"),  # optional if present\n",
    "        ]\n",
    "    else:  # spy\n",
    "        files = [\n",
    "            (\"options_snapshot_spy_clean.csv\", \"options_snapshot_spy\"),\n",
    "            (\"vol_surface_spy_clean.csv\",      \"vol_surface_spy\"),\n",
    "            (\"market_extended_spy_clean.csv\",  \"market_extended_spy\"), # optional if present\n",
    "        ]\n",
    "\n",
    "    # 1) row-preserving merge of each dataset with market\n",
    "    for fname, tag in files:\n",
    "        path = os.path.join(CLEANED, fname)\n",
    "        df = safe_load(path)\n",
    "        if df is None:\n",
    "            continue\n",
    "        merged = row_preserving_join_with_market(df, mkt)\n",
    "        merged.insert(0, \"source\", tag)\n",
    "        parts.append(merged)\n",
    "        sources.append(tag)\n",
    "        print(f\"‚úÖ {tag}: {merged.shape[0]:,} rows, {merged.shape[1]} cols\")\n",
    "\n",
    "    # 2) also include market_daily rows themselves (one row/day) if you want them in the unified file\n",
    "    mkt_rows = mkt.copy()\n",
    "    mkt_rows.insert(0, \"source\", \"market_daily\")\n",
    "    parts.append(mkt_rows)\n",
    "    sources.append(\"market_daily\")\n",
    "    print(f\"‚úÖ market_daily: {mkt_rows.shape[0]:,} rows, {mkt_rows.shape[1]} cols\")\n",
    "\n",
    "    # 3) union all rows, keeping ALL columns (outer-join on columns by concat)\n",
    "    if not parts:\n",
    "        raise RuntimeError(f\"No datasets found for {symbol.upper()}.\")\n",
    "    combined = pd.concat(parts, axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "    # Ensure 'date' is present & at front (and source second) for readability\n",
    "    cols = combined.columns.tolist()\n",
    "    ordered = [\"source\", \"date\"] + [c for c in cols if c not in (\"source\", \"date\")]\n",
    "    combined = combined[ordered]\n",
    "\n",
    "    # 4) save\n",
    "    out_csv = os.path.join(out_dir, f\"combined_{symbol}_all.csv\")\n",
    "    out_parq = os.path.join(out_dir, f\"combined_{symbol}_all.parquet\")\n",
    "    combined.to_csv(out_csv, index=False)\n",
    "    combined.to_parquet(out_parq, index=False)\n",
    "\n",
    "    print(f\"\\nüì¶ {symbol.upper()} combined saved:\")\n",
    "    print(f\"   - {out_csv}\")\n",
    "    print(f\"   - {out_parq}\")\n",
    "    print(f\"   Shape: {combined.shape[0]:,} rows √ó {combined.shape[1]} cols\")\n",
    "    print(f\"   Date range: {combined['date'].min()} ‚Üí {combined['date'].max()}\")\n",
    "    # quick per-source counts\n",
    "    print(\"\\nRow counts by source:\")\n",
    "    print(combined[\"source\"].value_counts().to_string())\n",
    "\n",
    "# ---------- build both ----------\n",
    "\n",
    "# SPX goes back to ~1996\n",
    "build_combined(symbol=\"spx\", start_date=\"1996-01-01\")\n",
    "\n",
    "# SPY starts ~2005\n",
    "build_combined(symbol=\"spy\", start_date=\"2005-01-01\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-hedging-rl)",
   "language": "python",
   "name": "deep-hedging-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
