{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD : /Users/ya/Desktop/deep-hedging-rl/notebooks\n",
      "ROOT: /Users/ya/Desktop/deep-hedging-rl\n",
      "SPX dir: /Users/ya/Desktop/deep-hedging-rl/data/raw/options_spx exists? True\n",
      "SPY dir: /Users/ya/Desktop/deep-hedging-rl/data/raw/options_spy exists? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/ya/Desktop/deep-hedging-rl/data/raw/options_spx/l8au9t4q1ij3lofm.csv'),\n",
       " PosixPath('/Users/ya/Desktop/deep-hedging-rl/data/raw/options_spy/nwymczfl7n3h33rg.csv'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "NB   = Path.cwd()\n",
    "ROOT = NB.parent\n",
    "SRC  = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.append(str(SRC))\n",
    "\n",
    "from data_pipeline.config import ROOT, RAW_SPX, RAW_SPY  # you showed these in your config\n",
    "\n",
    "print(\"CWD :\", Path.cwd())\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"SPX dir:\", RAW_SPX, \"exists?\", RAW_SPX.exists())\n",
    "print(\"SPY dir:\", RAW_SPY, \"exists?\", RAW_SPY.exists())\n",
    "\n",
    "def pick_csv(dirpath: Path) -> Path:\n",
    "    files = sorted(dirpath.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSVs in {dirpath}. Check the path or LFS checkout.\")\n",
    "    # choose the largest file if multiple\n",
    "    return max(files, key=lambda p: p.stat().st_size)\n",
    "\n",
    "SPX_FILE = pick_csv(RAW_SPX)\n",
    "SPY_FILE = pick_csv(RAW_SPY)\n",
    "SPX_FILE, SPY_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- config paths from your data_pipeline.config ---\n",
    "from pathlib import Path\n",
    "from data_pipeline.config import ROOT, RAW_SPX, RAW_SPY\n",
    "\n",
    "OUT_DIR = ROOT / \"data\" / \"processed\" / \"options_parquet\"\n",
    "(OUT_DIR / \"SPX\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"SPY\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OM_COLS = [\n",
    "    \"secid\",\"date\",\"exdate\",\"cp_flag\",\"strike_price\",\"best_bid\",\"best_offer\",\n",
    "    \"volume\",\"open_interest\",\"impl_volatility\",\"delta\",\"gamma\",\"vega\",\"theta\",\n",
    "    \"optionid\",\"root\",\"ticker\",\"index_flag\",\"issuer\",\"exercise_style\"\n",
    "]\n",
    "\n",
    "def csv_to_parquet_parts_om(csv_path: Path, out_dir: Path, chunksize=2_000_000):\n",
    "    import pandas as pd\n",
    "    part = 0\n",
    "    for chunk in pd.read_csv(\n",
    "        csv_path,\n",
    "        usecols=lambda c: c in OM_COLS,\n",
    "        parse_dates=[\"date\",\"exdate\"],\n",
    "        dtype={\n",
    "            \"cp_flag\":\"category\", \"ticker\":\"category\", \"root\":\"category\",\n",
    "            \"index_flag\":\"category\", \"exercise_style\":\"category\", \"issuer\":\"category\"\n",
    "        },\n",
    "        low_memory=False, chunksize=chunksize\n",
    "    ):\n",
    "        # --- normalize dates ---\n",
    "        for c in (\"date\",\"exdate\"):\n",
    "            chunk[c] = pd.to_datetime(chunk[c], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "        # --- rename to canonical ---\n",
    "        chunk = chunk.rename(columns={\n",
    "            \"exdate\":\"expiry\",\n",
    "            \"cp_flag\":\"put_call\",\n",
    "            \"strike_price\":\"strike\",\n",
    "            \"best_bid\":\"bid\",\n",
    "            \"best_offer\":\"ask\",\n",
    "            \"impl_volatility\":\"iv\",\n",
    "        })\n",
    "\n",
    "        # --- underlying ---\n",
    "        if \"underlying\" not in chunk:\n",
    "            chunk[\"underlying\"] = chunk[\"ticker\"].astype(\"string\")\n",
    "            # fallback if ticker missing:\n",
    "            mask = chunk[\"underlying\"].isna()\n",
    "            if mask.any() and \"root\" in chunk:\n",
    "                chunk.loc[mask, \"underlying\"] = chunk.loc[mask, \"root\"].astype(\"string\")\n",
    "\n",
    "        # --- compute mid if missing ---\n",
    "        chunk[\"mid\"] = (chunk[\"bid\"].astype(\"float32\") + chunk[\"ask\"].astype(\"float32\")) / 2.0\n",
    "\n",
    "        # --- strike unit auto-fix (handles ×1000 dumps) ---\n",
    "        s = pd.to_numeric(chunk[\"strike\"], errors=\"coerce\")\n",
    "        if s.max() and s.max() > 100000:   # heuristically detect milli-dollars\n",
    "            chunk[\"strike\"] = s / 1000.0\n",
    "        else:\n",
    "            chunk[\"strike\"] = s.astype(\"float32\")\n",
    "\n",
    "        # --- downcast numerics for memory ---\n",
    "        for c in (\"bid\",\"ask\",\"mid\",\"last\",\"iv\",\"delta\",\"gamma\",\"vega\",\"theta\"):\n",
    "            if c in chunk:\n",
    "                chunk[c] = pd.to_numeric(chunk[c], errors=\"coerce\", downcast=\"float\")\n",
    "        for c in (\"open_interest\",\"volume\"):\n",
    "            if c in chunk:\n",
    "                chunk[c] = pd.to_numeric(chunk[c], errors=\"coerce\", downcast=\"unsigned\")\n",
    "\n",
    "        # --- write this chunk as its own part ---\n",
    "        part += 1\n",
    "        chunk.to_parquet(out_dir / f\"part_{part:04d}.parquet\", index=False)\n",
    "\n",
    "# pick files and convert\n",
    "def pick_csv(dirpath: Path) -> Path:\n",
    "    files = sorted(dirpath.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSVs in {dirpath}\")\n",
    "    return max(files, key=lambda p: p.stat().st_size)\n",
    "\n",
    "SPX_FILE = pick_csv(RAW_SPX)\n",
    "SPY_FILE = pick_csv(RAW_SPY)\n",
    "\n",
    "csv_to_parquet_parts_om(SPX_FILE, OUT_DIR / \"SPX\")\n",
    "csv_to_parquet_parts_om(SPY_FILE, OUT_DIR / \"SPY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CANONICAL_ORDER = [\n",
    "    \"date\",\"underlying\",\"put_call\",\"expiry\",\"strike\",\n",
    "    \"bid\",\"ask\",\"mid\",\"last\",\"iv\",\"delta\",\"gamma\",\"vega\",\"theta\",\n",
    "    \"open_interest\",\"volume\",\"secid\",\"optionid\",\"root\",\"ticker\",\n",
    "    \"index_flag\",\"issuer\",\"exercise_style\"\n",
    "]\n",
    "\n",
    "def load_options_dir(dirpath: Path, columns=None) -> pd.DataFrame:\n",
    "    dset = ds.dataset(dirpath, format=\"parquet\")\n",
    "    tbl = dset.to_table(columns=None)  # all, we've already harmonized at write\n",
    "    df  = tbl.to_pandas()\n",
    "    # final guards\n",
    "    for c in (\"date\",\"expiry\"):\n",
    "        if c in df:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    # order columns (keep extras at end)\n",
    "    cols = [c for c in CANONICAL_ORDER if c in df.columns] + [c for c in df.columns if c not in CANONICAL_ORDER]\n",
    "    return df[cols]\n",
    "\n",
    "spx = load_options_dir(OUT_DIR / \"SPX\")\n",
    "spy = load_options_dir(OUT_DIR / \"SPY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPX] rows=32,190,466  1996-01-04→2023-08-31\n",
      "  neg_spread%=0.000%  zero_spread%=0.000%  iv>5%=0.039%\n",
      "[SPY] rows=20,357,886  2005-01-10→2023-08-31\n",
      "  neg_spread%=0.014%  zero_spread%=0.016%  iv>5%=0.025%\n",
      "Shared trading days: 4693\n"
     ]
    }
   ],
   "source": [
    "KEY = [\"date\",\"underlying\",\"put_call\",\"expiry\",\"strike\"]\n",
    "spx = spx.sort_values(KEY).drop_duplicates(KEY, keep=\"last\")\n",
    "spy = spy.sort_values(KEY).drop_duplicates(KEY, keep=\"last\")\n",
    "\n",
    "def qc(df, name):\n",
    "    print(f\"[{name}] rows={len(df):,}  {df['date'].min().date()}→{df['date'].max().date()}\")\n",
    "    s = (df[\"ask\"] - df[\"bid\"])\n",
    "    print(f\"  neg_spread%={(s<0).mean():.3%}  zero_spread%={(s==0).mean():.3%}  iv>5%={(df['iv']>5).mean():.3%}\")\n",
    "\n",
    "qc(spx, \"SPX\"); qc(spy, \"SPY\")\n",
    "\n",
    "dates_spx = set(spx[\"date\"]); dates_spy = set(spy[\"date\"])\n",
    "print(\"Shared trading days:\", len(dates_spx & dates_spy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --- paths (use your config) ---\n",
    "from data_pipeline.config import ROOT, PROCESSED_DIR\n",
    "SRC_BASE = ROOT / \"data\" / \"processed\" / \"options_parquet\"\n",
    "DST_BASE = ROOT / \"data\" / \"processed\" / \"cleaned\"\n",
    "\n",
    "DST_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- canonical columns to KEEP ---\n",
    "KEEP = [\n",
    "    \"date\",\"underlying\",\"put_call\",\"expiry\",\"strike\",\n",
    "    \"bid\",\"ask\",\"mid\",\"iv\",\"delta\",\"gamma\",\"vega\",\"theta\",\n",
    "    \"open_interest\",\"volume\",\"tenor_d\"\n",
    "]\n",
    "\n",
    "# --- aliases we may see from OptionMetrics ---\n",
    "ALIASES = {\n",
    "    \"exdate\": \"expiry\",\n",
    "    \"cp_flag\": \"put_call\",\n",
    "    \"strike_price\": \"strike\",\n",
    "    \"best_bid\": \"bid\",\n",
    "    \"best_offer\": \"ask\",\n",
    "    \"impl_volatility\": \"iv\",\n",
    "}\n",
    "\n",
    "# we might read some metadata to build \"underlying\" if needed\n",
    "META_CANDIDATES = [\"ticker\",\"root\",\"exercise_style\",\"index_flag\",\"issuer\"]\n",
    "\n",
    "# read candidates = keep + aliases keys + meta\n",
    "CANDIDATES = set(KEEP) | set(ALIASES.keys()) | set(META_CANDIDATES)\n",
    "\n",
    "def _read_part_selective(path: Path) -> pd.DataFrame:\n",
    "    # list present columns without loading the whole file\n",
    "    pf = pq.ParquetFile(path)\n",
    "    names = set(pf.schema.names)\n",
    "    cols = [c for c in CANDIDATES if c in names]\n",
    "    tbl = pf.read(columns=cols)\n",
    "    return tbl.to_pandas()\n",
    "\n",
    "def _clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    x = df.rename(columns={k:v for k,v in ALIASES.items() if k in df.columns}).copy()\n",
    "\n",
    "    # underlying\n",
    "    if \"underlying\" not in x.columns:\n",
    "        if \"ticker\" in x: x[\"underlying\"] = x[\"ticker\"].astype(\"string\")\n",
    "        elif \"root\" in x: x[\"underlying\"] = x[\"root\"].astype(\"string\")\n",
    "    x[\"underlying\"] = x[\"underlying\"].str.upper()\n",
    "\n",
    "    # dates\n",
    "    for c in (\"date\",\"expiry\"):\n",
    "        if c in x.columns:\n",
    "            x[c] = pd.to_datetime(x[c], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "    # mid\n",
    "    if \"mid\" not in x.columns and {\"bid\",\"ask\"} <= set(x.columns):\n",
    "        x[\"mid\"] = (pd.to_numeric(x[\"bid\"], errors=\"coerce\") +\n",
    "                    pd.to_numeric(x[\"ask\"], errors=\"coerce\")) / 2.0\n",
    "\n",
    "    # strike unit sanity (some dumps x1000)\n",
    "    if \"strike\" in x.columns:\n",
    "        s = pd.to_numeric(x[\"strike\"], errors=\"coerce\")\n",
    "        x[\"strike\"] = (s/1000.0) if (s.max() and s.max() > 100_000) else s.astype(\"float32\")\n",
    "\n",
    "    # tenor + hygiene filters\n",
    "    x[\"tenor_d\"] = (x[\"expiry\"] - x[\"date\"]).dt.days\n",
    "    x = x[(x[\"bid\"] > 0) & (x[\"ask\"] > 0) & (x[\"ask\"] >= x[\"bid\"])]\n",
    "    x = x[(x[\"iv\"] > 0) & (x[\"iv\"] <= 5.0)]\n",
    "    x = x[(x[\"delta\"] >= -1.05) & (x[\"delta\"] <= 1.05)]\n",
    "    x = x[(x[\"tenor_d\"] >= 4) & (x[\"tenor_d\"] <= 730)]\n",
    "\n",
    "    # downcast for size\n",
    "    for c in (\"bid\",\"ask\",\"mid\",\"iv\",\"delta\",\"gamma\",\"vega\",\"theta\"):\n",
    "        if c in x: x[c] = pd.to_numeric(x[c], errors=\"coerce\", downcast=\"float\")\n",
    "    for c in (\"open_interest\",\"volume\",\"tenor_d\"):\n",
    "        if c in x: x[c] = pd.to_numeric(x[c], errors=\"coerce\", downcast=\"unsigned\")\n",
    "\n",
    "    # final select/order\n",
    "    keep = [c for c in KEEP if c in x.columns]\n",
    "    return x[keep].reset_index(drop=True)\n",
    "\n",
    "def clean_options_dir(symbol: str, strict_liquidity: bool = False) -> Path:\n",
    "    src_dir = SRC_BASE / symbol\n",
    "    dst_dir = DST_BASE / symbol\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    parts = sorted(src_dir.glob(\"part_*.parquet\"))\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(f\"No parquet parts in {src_dir}\")\n",
    "\n",
    "    out_i = 0\n",
    "    for p in parts:\n",
    "        chunk = _read_part_selective(p)\n",
    "        chunk = _clean_chunk(chunk)\n",
    "\n",
    "        if strict_liquidity:\n",
    "            if {\"open_interest\",\"volume\"}.issubset(chunk.columns):\n",
    "                chunk = chunk[(chunk[\"open_interest\"] > 0) & (chunk[\"volume\"] > 0)]\n",
    "\n",
    "        out_i += 1\n",
    "        chunk.to_parquet(dst_dir / f\"part_{out_i:04d}.parquet\",\n",
    "                         index=False, compression=\"zstd\")\n",
    "\n",
    "    # quick manifest\n",
    "    import pyarrow.dataset as ds\n",
    "    dset = ds.dataset(dst_dir, format=\"parquet\")\n",
    "    tbl  = dset.to_table(columns=[\"date\"])\n",
    "    df   = tbl.to_pandas()\n",
    "    summary = {\n",
    "        \"symbol\": symbol,\n",
    "        \"parts\": out_i,\n",
    "        \"rows\": len(df),\n",
    "        \"date_min\": str(df[\"date\"].min().date()) if len(df) else None,\n",
    "        \"date_max\": str(df[\"date\"].max().date()) if len(df) else None,\n",
    "        \"path\": str(dst_dir),\n",
    "    }\n",
    "    pd.Series(summary).to_json(dst_dir / \"_manifest.json\", indent=2)\n",
    "    return dst_dir\n",
    "\n",
    "spx_clean_dir = clean_options_dir(\"SPX\", strict_liquidity=False)\n",
    "spy_clean_dir = clean_options_dir(\"SPY\", strict_liquidity=False)\n",
    "spx_clean_dir, spy_clean_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/ya/Desktop/deep-hedging-rl/data/processed/cleaned/SPX'),\n",
       " PosixPath('/Users/ya/Desktop/deep-hedging-rl/data/processed/cleaned/SPY'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spx_clean_dir = clean_options_dir(\"SPX\", strict_liquidity=False)\n",
    "spy_clean_dir = clean_options_dir(\"SPY\", strict_liquidity=False)\n",
    "spx_clean_dir, spy_clean_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking SPX files for NaNs:\n",
      "File: part_0001.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0002.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0003.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0004.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0005.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0006.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0007.parquet\n",
      "theta    True\n",
      "dtype: bool\n",
      "----------------------------------------\n",
      "File: part_0008.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0009.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0010.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0011.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0012.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0013.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0014.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0015.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0016.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0017.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0018.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0019.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "\n",
      "Checking SPY files for NaNs:\n",
      "File: part_0001.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0002.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0003.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0004.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0005.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0006.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0007.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0008.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0009.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0010.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n",
      "File: part_0011.parquet\n",
      "Series([], dtype: bool)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Function to check for NaNs in a parquet file\n",
    "def check_nans_in_parquet(directory):\n",
    "    parquet_files = sorted(directory.glob(\"part_*.parquet\"))\n",
    "    for file in parquet_files:\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        nan_summary = df.isna().sum()\n",
    "        print(f\"File: {file.name}\")\n",
    "        print(nan_summary[nan_summary > 0])  # Show only columns with NaNs\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Check for NaNs in SPX and SPY parquet files\n",
    "print(\"Checking SPX files for NaNs:\")\n",
    "check_nans_in_parquet(spx_clean_dir)\n",
    "\n",
    "print(\"\\nChecking SPY files for NaNs:\")\n",
    "check_nans_in_parquet(spy_clean_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
