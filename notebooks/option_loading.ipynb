{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD : /Users/ya/Desktop/deep-hedging-rl/notebooks\n",
      "ROOT: /Users/ya/Desktop/deep-hedging-rl\n",
      "SPX dir: /Users/ya/Desktop/deep-hedging-rl/data/raw/options_spx exists? True\n",
      "SPY dir: /Users/ya/Desktop/deep-hedging-rl/data/raw/options_spy exists? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/ya/Desktop/deep-hedging-rl/data/raw/options_spx/l8au9t4q1ij3lofm.csv'),\n",
       " PosixPath('/Users/ya/Desktop/deep-hedging-rl/data/raw/options_spy/nwymczfl7n3h33rg.csv'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "NB   = Path.cwd()\n",
    "ROOT = NB.parent\n",
    "SRC  = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.append(str(SRC))\n",
    "\n",
    "from pathlib import Path\n",
    "from data_pipeline.config import ROOT, RAW_SPX, RAW_SPY  # you showed these in your config\n",
    "\n",
    "print(\"CWD :\", Path.cwd())\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"SPX dir:\", RAW_SPX, \"exists?\", RAW_SPX.exists())\n",
    "print(\"SPY dir:\", RAW_SPY, \"exists?\", RAW_SPY.exists())\n",
    "\n",
    "def pick_csv(dirpath: Path) -> Path:\n",
    "    files = sorted(dirpath.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSVs in {dirpath}. Check the path or LFS checkout.\")\n",
    "    # choose the largest file if multiple\n",
    "    return max(files, key=lambda p: p.stat().st_size)\n",
    "\n",
    "SPX_FILE = pick_csv(RAW_SPX)\n",
    "SPY_FILE = pick_csv(RAW_SPY)\n",
    "SPX_FILE, SPY_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- config paths from your data_pipeline.config ---\n",
    "from pathlib import Path\n",
    "from data_pipeline.config import ROOT, RAW_SPX, RAW_SPY\n",
    "\n",
    "OUT_DIR = ROOT / \"data\" / \"processed\" / \"options_parquet\"\n",
    "(OUT_DIR / \"SPX\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"SPY\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OM_COLS = [\n",
    "    \"secid\",\"date\",\"exdate\",\"cp_flag\",\"strike_price\",\"best_bid\",\"best_offer\",\n",
    "    \"volume\",\"open_interest\",\"impl_volatility\",\"delta\",\"gamma\",\"vega\",\"theta\",\n",
    "    \"optionid\",\"root\",\"ticker\",\"index_flag\",\"issuer\",\"exercise_style\"\n",
    "]\n",
    "\n",
    "def csv_to_parquet_parts_om(csv_path: Path, out_dir: Path, chunksize=2_000_000):\n",
    "    import pandas as pd\n",
    "    part = 0\n",
    "    for chunk in pd.read_csv(\n",
    "        csv_path,\n",
    "        usecols=lambda c: c in OM_COLS,\n",
    "        parse_dates=[\"date\",\"exdate\"],\n",
    "        dtype={\n",
    "            \"cp_flag\":\"category\", \"ticker\":\"category\", \"root\":\"category\",\n",
    "            \"index_flag\":\"category\", \"exercise_style\":\"category\", \"issuer\":\"category\"\n",
    "        },\n",
    "        low_memory=False, chunksize=chunksize\n",
    "    ):\n",
    "        # --- normalize dates ---\n",
    "        for c in (\"date\",\"exdate\"):\n",
    "            chunk[c] = pd.to_datetime(chunk[c], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "        # --- rename to canonical ---\n",
    "        chunk = chunk.rename(columns={\n",
    "            \"exdate\":\"expiry\",\n",
    "            \"cp_flag\":\"put_call\",\n",
    "            \"strike_price\":\"strike\",\n",
    "            \"best_bid\":\"bid\",\n",
    "            \"best_offer\":\"ask\",\n",
    "            \"impl_volatility\":\"iv\",\n",
    "        })\n",
    "\n",
    "        # --- underlying ---\n",
    "        if \"underlying\" not in chunk:\n",
    "            chunk[\"underlying\"] = chunk[\"ticker\"].astype(\"string\")\n",
    "            # fallback if ticker missing:\n",
    "            mask = chunk[\"underlying\"].isna()\n",
    "            if mask.any() and \"root\" in chunk:\n",
    "                chunk.loc[mask, \"underlying\"] = chunk.loc[mask, \"root\"].astype(\"string\")\n",
    "\n",
    "        # --- compute mid if missing ---\n",
    "        chunk[\"mid\"] = (chunk[\"bid\"].astype(\"float32\") + chunk[\"ask\"].astype(\"float32\")) / 2.0\n",
    "\n",
    "        # --- strike unit auto-fix (handles ×1000 dumps) ---\n",
    "        s = pd.to_numeric(chunk[\"strike\"], errors=\"coerce\")\n",
    "        if s.max() and s.max() > 100000:   # heuristically detect milli-dollars\n",
    "            chunk[\"strike\"] = s / 1000.0\n",
    "        else:\n",
    "            chunk[\"strike\"] = s.astype(\"float32\")\n",
    "\n",
    "        # --- downcast numerics for memory ---\n",
    "        for c in (\"bid\",\"ask\",\"mid\",\"last\",\"iv\",\"delta\",\"gamma\",\"vega\",\"theta\"):\n",
    "            if c in chunk:\n",
    "                chunk[c] = pd.to_numeric(chunk[c], errors=\"coerce\", downcast=\"float\")\n",
    "        for c in (\"open_interest\",\"volume\"):\n",
    "            if c in chunk:\n",
    "                chunk[c] = pd.to_numeric(chunk[c], errors=\"coerce\", downcast=\"unsigned\")\n",
    "\n",
    "        # --- write this chunk as its own part ---\n",
    "        part += 1\n",
    "        chunk.to_parquet(out_dir / f\"part_{part:04d}.parquet\", index=False)\n",
    "\n",
    "# pick files and convert\n",
    "def pick_csv(dirpath: Path) -> Path:\n",
    "    files = sorted(dirpath.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSVs in {dirpath}\")\n",
    "    return max(files, key=lambda p: p.stat().st_size)\n",
    "\n",
    "SPX_FILE = pick_csv(RAW_SPX)\n",
    "SPY_FILE = pick_csv(RAW_SPY)\n",
    "\n",
    "csv_to_parquet_parts_om(SPX_FILE, OUT_DIR / \"SPX\")\n",
    "csv_to_parquet_parts_om(SPY_FILE, OUT_DIR / \"SPY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CANONICAL_ORDER = [\n",
    "    \"date\",\"underlying\",\"put_call\",\"expiry\",\"strike\",\n",
    "    \"bid\",\"ask\",\"mid\",\"last\",\"iv\",\"delta\",\"gamma\",\"vega\",\"theta\",\n",
    "    \"open_interest\",\"volume\",\"secid\",\"optionid\",\"root\",\"ticker\",\n",
    "    \"index_flag\",\"issuer\",\"exercise_style\"\n",
    "]\n",
    "\n",
    "def load_options_dir(dirpath: Path, columns=None) -> pd.DataFrame:\n",
    "    dset = ds.dataset(dirpath, format=\"parquet\")\n",
    "    tbl = dset.to_table(columns=None)  # all, we've already harmonized at write\n",
    "    df  = tbl.to_pandas()\n",
    "    # final guards\n",
    "    for c in (\"date\",\"expiry\"):\n",
    "        if c in df:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "    # order columns (keep extras at end)\n",
    "    cols = [c for c in CANONICAL_ORDER if c in df.columns] + [c for c in df.columns if c not in CANONICAL_ORDER]\n",
    "    return df[cols]\n",
    "\n",
    "spx = load_options_dir(OUT_DIR / \"SPX\")\n",
    "spy = load_options_dir(OUT_DIR / \"SPY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPX] rows=32,190,466  1996-01-04→2023-08-31\n",
      "  neg_spread%=0.000%  zero_spread%=0.000%  iv>5%=0.039%\n",
      "[SPY] rows=20,357,886  2005-01-10→2023-08-31\n",
      "  neg_spread%=0.014%  zero_spread%=0.016%  iv>5%=0.025%\n",
      "Shared trading days: 4693\n"
     ]
    }
   ],
   "source": [
    "KEY = [\"date\",\"underlying\",\"put_call\",\"expiry\",\"strike\"]\n",
    "spx = spx.sort_values(KEY).drop_duplicates(KEY, keep=\"last\")\n",
    "spy = spy.sort_values(KEY).drop_duplicates(KEY, keep=\"last\")\n",
    "\n",
    "def qc(df, name):\n",
    "    print(f\"[{name}] rows={len(df):,}  {df['date'].min().date()}→{df['date'].max().date()}\")\n",
    "    s = (df[\"ask\"] - df[\"bid\"])\n",
    "    print(f\"  neg_spread%={(s<0).mean():.3%}  zero_spread%={(s==0).mean():.3%}  iv>5%={(df['iv']>5).mean():.3%}\")\n",
    "\n",
    "qc(spx, \"SPX\"); qc(spy, \"SPY\")\n",
    "\n",
    "dates_spx = set(spx[\"date\"]); dates_spy = set(spy[\"date\"])\n",
    "print(\"Shared trading days:\", len(dates_spx & dates_spy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
